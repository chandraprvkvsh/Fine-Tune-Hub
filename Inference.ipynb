{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 5111,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3899
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**The Fine-Tuning was done in a different notebook, The tuned model adapters were saved to huggingface which we will use here for inference. Feel free to use your own tuned adapters, We will update the adapters as we train the model on more data and do RLHF.**"
      ],
      "metadata": {
        "id": "RoCpjG6F0cvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the saved model**"
      ],
      "metadata": {
        "id": "OhuR_JPd0cvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git  -U\n",
        "!pip install -q bitsandbytes accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git  -U\n",
        "!pip install -q -U datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-14T11:29:32.387895Z",
          "iopub.execute_input": "2024-03-14T11:29:32.388262Z",
          "iopub.status.idle": "2024-03-14T11:31:49.440627Z",
          "shell.execute_reply.started": "2024-03-14T11:29:32.388232Z",
          "shell.execute_reply": "2024-03-14T11:31:49.439437Z"
        },
        "trusted": true,
        "id": "Zi_uvflV0cv1",
        "outputId": "646b5c12-61a6-48ce-cc75-fd2d0818d4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets==2.16.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:31:49.443069Z",
          "iopub.execute_input": "2024-03-14T11:31:49.443496Z",
          "iopub.status.idle": "2024-03-14T11:32:05.047761Z",
          "shell.execute_reply.started": "2024-03-14T11:31:49.443433Z",
          "shell.execute_reply": "2024-03-14T11:32:05.046616Z"
        },
        "trusted": true,
        "id": "NgM7R2md0cv3",
        "outputId": "7f2f961a-8053-44bb-ee87-b15493700ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (15.0.1)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (2024.2.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.16.0)\nDownloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, dill, multiprocess, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.2.0\n    Uninstalling fsspec-2024.2.0:\n      Successfully uninstalled fsspec-2024.2.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os,torch, wandb\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:32:05.049287Z",
          "iopub.execute_input": "2024-03-14T11:32:05.049630Z",
          "iopub.status.idle": "2024-03-14T11:32:25.374679Z",
          "shell.execute_reply.started": "2024-03-14T11:32:05.049603Z",
          "shell.execute_reply": "2024-03-14T11:32:25.373682Z"
        },
        "trusted": true,
        "id": "Ohsm0GcC0cv4",
        "outputId": "e61edc6c-bc67-4cac-a464-eea872c5e2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-14 11:32:14.245360: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 11:32:14.245507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 11:32:14.399982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel=\"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
        "tunedmodel=\"KaggleMasterX/mistral7b_0403\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:32:25.376817Z",
          "iopub.execute_input": "2024-03-14T11:32:25.377415Z",
          "iopub.status.idle": "2024-03-14T11:32:25.383431Z",
          "shell.execute_reply.started": "2024-03-14T11:32:25.377386Z",
          "shell.execute_reply": "2024-03-14T11:32:25.382525Z"
        },
        "trusted": true,
        "id": "L7B59i8R0cv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(basemodel)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    basemodel,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map= \"auto\",\n",
        "    low_cpu_mem_usage = True,\n",
        ")\n",
        "\n",
        "# KV cache is useless during training(Finetune), It only works for inference.\n",
        "model.config.use_cache = False # enable this in inference mode\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:32:25.384919Z",
          "iopub.execute_input": "2024-03-14T11:32:25.385342Z"
        },
        "trusted": true,
        "id": "7ogsiR_J0cv4",
        "outputId": "7e7560c7-927c-40e1-a3cf-5ca5f73be48e",
        "colab": {
          "referenced_widgets": [
            "13920090778e4578b972aad94c0be746"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13920090778e4578b972aad94c0be746"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import *\n",
        "peft_model_id = tunedmodel\n",
        "config = PeftConfig.from_pretrained(tunedmodel)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    basemodel,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(basemodel)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.idle": "2024-03-14T11:34:19.165576Z",
          "shell.execute_reply.started": "2024-03-14T11:34:02.868668Z",
          "shell.execute_reply": "2024-03-14T11:34:19.164708Z"
        },
        "trusted": true,
        "id": "M6XjWeTO0cv5",
        "outputId": "70bcc80d-0813-44ba-c4bd-125537d6c503",
        "colab": {
          "referenced_widgets": [
            "86190c42910d4d32a1c3276392e59494"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86190c42910d4d32a1c3276392e59494"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:34:19.166814Z",
          "iopub.execute_input": "2024-03-14T11:34:19.167146Z",
          "iopub.status.idle": "2024-03-14T11:34:19.181281Z",
          "shell.execute_reply.started": "2024-03-14T11:34:19.167120Z",
          "shell.execute_reply": "2024-03-14T11:34:19.180406Z"
        },
        "trusted": true,
        "id": "Sw5SyZCm0cv5",
        "outputId": "03f18d64-847b-4572-d457-9733e6a0eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModel(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Replace abstract as your desired abstract and the run the cells after it**"
      ],
      "metadata": {
        "id": "rdA9F-MK0cv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction=\"Craft an intelligent, clear, insightful, and succinct one-line title for the research paper, drawing inspiration from the abstract provided. \\n\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:34:19.182374Z",
          "iopub.execute_input": "2024-03-14T11:34:19.182670Z",
          "iopub.status.idle": "2024-03-14T11:34:19.192011Z",
          "shell.execute_reply.started": "2024-03-14T11:34:19.182646Z",
          "shell.execute_reply": "2024-03-14T11:34:19.191155Z"
        },
        "trusted": true,
        "id": "FuT-Lc-C0cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Input**"
      ],
      "metadata": {
        "id": "D5Oyg9wm0cv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract=\"Introducing YOLO, a revolutionary approach to object detection that breaks away from traditional methods. Instead of repurposing classifiers, YOLO tackles object detection as a regression problem, focusing on spatially separated bounding boxes and their associated class probabilities. With a single neural network, YOLO directly predicts bounding boxes and class probabilities from entire images in one swift evaluation. This unified network design allows for end-to-end optimization, streamlining the detection process. The efficiency of our architecture is remarkable. The base YOLO model achieves real-time image processing at 45 frames per second. Even a scaled-down variant, Fast YOLO, operates at an impressive 155 frames per second while doubling the mean Average Precision (mAP) compared to other real-time detectors. Though YOLO may exhibit more localization errors compared to state-of-the-art systems, its false detection rate is significantly lower, minimizing erroneous predictions in void areas. Furthermore, YOLO demonstrates remarkable adaptability, learning general object representations. It surpasses other detection methods, such as DPM and R-CNN, by a considerable margin, particularly evident when generalizing from natural images to artwork datasets like the Picasso Dataset and the People-Art Dataset.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:34:19.193030Z",
          "iopub.execute_input": "2024-03-14T11:34:19.193308Z",
          "iopub.status.idle": "2024-03-14T11:34:19.202523Z",
          "shell.execute_reply.started": "2024-03-14T11:34:19.193285Z",
          "shell.execute_reply": "2024-03-14T11:34:19.201711Z"
        },
        "trusted": true,
        "id": "IrJN8xzy0cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=instruction+abstract"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:34:19.205043Z",
          "iopub.execute_input": "2024-03-14T11:34:19.205319Z",
          "iopub.status.idle": "2024-03-14T11:34:19.215909Z",
          "shell.execute_reply.started": "2024-03-14T11:34:19.205297Z",
          "shell.execute_reply": "2024-03-14T11:34:19.214947Z"
        },
        "trusted": true,
        "id": "QNiKipVI0cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ouptut**"
      ],
      "metadata": {
        "id": "qlpaj2rZ0cv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "device = \"cuda\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "temp_out=(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:34:19.217019Z",
          "iopub.execute_input": "2024-03-14T11:34:19.217296Z",
          "iopub.status.idle": "2024-03-14T11:34:31.909831Z",
          "shell.execute_reply.started": "2024-03-14T11:34:19.217273Z",
          "shell.execute_reply": "2024-03-14T11:34:31.909022Z"
        },
        "trusted": true,
        "id": "3zF3C9030cv7",
        "outputId": "3c362790-7a10-4c5f-a3c0-e0f816427bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = temp_out.split(\"[/INST]\")[-1].strip()\n",
        "result[:80]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:53:04.580142Z",
          "iopub.execute_input": "2024-03-14T11:53:04.581025Z",
          "iopub.status.idle": "2024-03-14T11:53:04.586896Z",
          "shell.execute_reply.started": "2024-03-14T11:53:04.580993Z",
          "shell.execute_reply": "2024-03-14T11:53:04.585985Z"
        },
        "trusted": true,
        "id": "BFKsheBv0cv7",
        "outputId": "f872175d-4221-4ad0-9ad8-19a531969145"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'you only look once: unified, real-time object detection 1000x faster than r-cnn '"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Input**"
      ],
      "metadata": {
        "id": "uAchz6xW0cv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract=\"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:54:11.691521Z",
          "iopub.execute_input": "2024-03-14T11:54:11.692533Z",
          "iopub.status.idle": "2024-03-14T11:54:11.697743Z",
          "shell.execute_reply.started": "2024-03-14T11:54:11.692488Z",
          "shell.execute_reply": "2024-03-14T11:54:11.696726Z"
        },
        "trusted": true,
        "id": "ssjBnyW50cv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=instruction+abstract\n",
        "\n",
        "text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "device = \"cuda\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "temp_out=(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:54:16.166499Z",
          "iopub.execute_input": "2024-03-14T11:54:16.167377Z",
          "iopub.status.idle": "2024-03-14T11:54:26.150240Z",
          "shell.execute_reply.started": "2024-03-14T11:54:16.167342Z",
          "shell.execute_reply": "2024-03-14T11:54:26.149455Z"
        },
        "trusted": true,
        "id": "cU1A5bms0cv8",
        "outputId": "f8666d8b-4d79-4a98-84f7-a13d289317cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = temp_out.split(\"[/INST]\")[-1].strip()\n",
        "result[:42]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-14T11:54:56.424403Z",
          "iopub.execute_input": "2024-03-14T11:54:56.425371Z",
          "iopub.status.idle": "2024-03-14T11:54:56.431782Z",
          "shell.execute_reply.started": "2024-03-14T11:54:56.425332Z",
          "shell.execute_reply": "2024-03-14T11:54:56.430726Z"
        },
        "trusted": true,
        "id": "GSKhBKz80cv8",
        "outputId": "f1468f44-a4d4-4101-b520-5913024c45e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'adam: a method for stochastic optimization'"
          },
          "metadata": {}
        }
      ]
    }
  ]
}